# Review

apache spark를 docker 기반 stand alone클러스터로 구성하고, spark job을 실행하는 과정을 실습했다. 이 과정에서 내가 중점적으로 생각했던 부분은 map-reduce와 spark가 어떻게 다르게 동작하는지에 대한 것이다. map-reduce가 있는데 왜 spark가 등장했고, 또 spark가 존재하지만 왜 map-reduce를 아직까지 사용할지에 대해서 생각을 해 보았다. 그 결과 이것도 사용 환경에 따라 결정되는 요소라고 생각한다. map reduce는 메모리 의존도가 낮기 때문에 단순한 작업들을 싼 값에 수행할 수 있다는 장점이 있을 것 같다. 반면에 spark가 딥러닝/머신러닝 모델들을 서빙할 때 주로 사용하는 이유는 또한 딥러닝/머신러닝 알고리즘들은 연산량이 많기 때문이라는 것을 아키텍쳐 측면에서 배울 수 있었다.

---
# Retrospect

## Keep
- spark와 map-reduce에 대한 설명을 다시한번 정리해 구현한 것
    - map reduce에서 yarn의 역할과 spark에서 yarn의 역할 등

- 미션 수행 중 문제의 애매한 설명들을 팀원들과 함께 토의해서 명확히 하고 미션들을 수행한 것

### Problem
- 지난주에 하둡과 map reduce의 개념이 잘 이해가 되지 않아서, mission을 진행하면서 아키텍쳐와 진행과정을 공부해 나갔었다. 하지만, 미션이 끝나고 나서 당시에 공부했던 내용을 따로 정리하지 않고 spark를 보니까 헷갈리는 부분들이 좀 있었던거 같다. 앞으로는 mission을 끝내고 금요일(15시-18시)에는 이번주 공부한 내용을 다시 정리하고 넘어가는 시간을 가져야 겠다.